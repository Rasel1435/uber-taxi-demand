{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature_Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from feature_engine.datetime import DatetimeFeatures\n",
    "from feature_engine.selection import SmartCorrelatedSelection, RecursiveFeatureElimination\n",
    "from feature_engine.timeseries.forecasting import LagFeatures, WindowFeatures, ExpandingWindowFeatures\n",
    "\n",
    "### Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt= '%d-%b(%m)-%Y %I:%M:%S',\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#Lodding Data\n",
    "def getData(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"in getData(): {e}\")\n",
    "\n",
    "def dataCleaning() -> None:\n",
    "    global df\n",
    "    try:\n",
    "        df['timestamp'] = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "        df.drop(columns=['tpep_pickup_datetime'], inplace= True)\n",
    "        df.drop_duplicates(subset= ['timestamp'], inplace=True)\n",
    "        df = df[~(df.timestamp > pd.Timestamp('2022-12-31 00:00:00'))]\n",
    "    except Exception as e:\n",
    "        logger.error(f'in dataCleaning(): {e}')\n",
    "        \n",
    "# featureEngineering start here\n",
    "def add_temporal_features() -> None:\n",
    "    global df\n",
    "    try:\n",
    "        features_to_extract = [\n",
    "            \"month\", \"quarter\", \"semester\", \"year\", \"week\", \"day_of_week\", \"day_of_month\",\n",
    "            \"day_of_year\", \"weekend\", \"month_start\", \"month_end\", \"quarter_start\",\n",
    "            \"quarter_end\", \"year_start\", \"year_end\", \"leap_year\", \"days_in_month\", \"hour\", \"minute\", \"second\"\n",
    "        ]\n",
    "        temporal = DatetimeFeatures(features_to_extract=features_to_extract).fit_transform(df[['timestamp']])\n",
    "        for col in temporal.columns:\n",
    "            df.loc[:, col] = temporal[col].values\n",
    "    except Exception as e:\n",
    "        logger.error(f'in add_temporal_features(): {e}')\n",
    "\n",
    "def add_lag_features() -> None:\n",
    "    global df\n",
    "    try:\n",
    "        lagfeatures = LagFeatures(variables=None, periods=[1, 2, 4, 8, 16, 24], freq=None, sort_index=True,\n",
    "                                missing_values='raise', drop_original=False)\n",
    "        lagfeatures.fit(df[['timestamp', 'passenger_demand', 'taxi_demand']])\n",
    "        features = lagfeatures.transform(df[['timestamp', 'passenger_demand', 'taxi_demand']])\n",
    "        for col in list(features.columns)[3:]:\n",
    "            df[col] = features[col].values\n",
    "    except Exception as e:\n",
    "        logger.error(f'in The add_lag_features(): {e}')\n",
    "        \n",
    "def add_window_features() -> None:\n",
    "    global df\n",
    "    try:\n",
    "        window = WindowFeatures(\n",
    "            variables=None, window=7, min_periods=1,\n",
    "            functions=['mean', 'std', 'median'], periods=1, freq=None, sort_index=True,\n",
    "            missing_values='raise', drop_original=False\n",
    "        )\n",
    "        window.fit(df[['timestamp', 'passenger_demand', 'taxi_demand']])\n",
    "        features = window.fit_transform(df[['timestamp', 'passenger_demand', 'taxi_demand']])\n",
    "        for col in list(features.columns)[3:]:\n",
    "            df[col] = features[col].values\n",
    "    except Exception as e:\n",
    "        logger.error(f'in add_window_features(): {e}')\n",
    "        \n",
    "def add_exp_window_features() -> None:\n",
    "    global df\n",
    "    try:\n",
    "        expwindow = ExpandingWindowFeatures(\n",
    "            variables=None, min_periods=None, functions='std',\n",
    "            periods=1, freq=None, sort_index=True,\n",
    "            missing_values='raise', drop_original=False\n",
    "        )\n",
    "        expwindow.fit(df[['timestamp', 'passenger_demand', 'taxi_demand']])\n",
    "        features = expwindow.fit_transform(df[['timestamp', 'passenger_demand', 'taxi_demand']])\n",
    "        for col in list(features.columns)[3:]:\n",
    "            df[col] = features[col].values\n",
    "    except Exception as e:\n",
    "        logger.error(f'in add_exp_window_features(): {e}')\n",
    "        \n",
    "# Feture Selection Start here\n",
    "def select_best_features():\n",
    "    global df\n",
    "    try:\n",
    "        X = df.drop(columns=['timestamp','passenger_demand', 'taxi_demand'])\n",
    "        y = df['taxi_demand']\n",
    "        scs = SmartCorrelatedSelection(\n",
    "            variables=None, method='pearson', threshold=0.5,\n",
    "            missing_values='ignore', selection_method='variance',\n",
    "            confirm_variables=False\n",
    "        )\n",
    "        scs_columns = set(scs.fit_transform(X).columns)\n",
    "        rfe = RecursiveFeatureElimination(\n",
    "            DecisionTreeRegressor(max_depth=3), scoring='r2', cv=3, threshold=0.01,\n",
    "            variables=None, confirm_variables=False\n",
    "        )\n",
    "        rfe_columns = rfe.fit_transform(X, y)\n",
    "        scs_columns.update(rfe_columns)\n",
    "        df = df[list(scs_columns)]\n",
    "        df['taxi_demand'] = y\n",
    "    except Exception as e:\n",
    "        logger.error(f'in select_best_features(): {e}')\n",
    "\n",
    "# Data Scaling here\n",
    "def normalizeScaling() -> None:\n",
    "    global df\n",
    "    try:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df.drop(columns=['taxi_demand',]))\n",
    "        df.loc[:, df.columns[:-1]] = scaler.transform(df.drop(columns=['taxi_demand',])) \n",
    "    except Exception as e:\n",
    "        logger.error(f\"in normalizeScaling(): {e}\")\n",
    "        \n",
    "# DimensonalRedaction start from here\n",
    "def reduceDimensionality() -> None:\n",
    "    global df\n",
    "    try:\n",
    "        features = df.drop(columns=['taxi_demand'])\n",
    "        target = df['taxi_demand']\n",
    "        pca = PCA(n_components=19)\n",
    "        features_reduced = pca.fit_transform(features)\n",
    "        df = pd.DataFrame(features_reduced, columns=[f'PC{i}' for i in range(1, 20)])\n",
    "        df['taxi_demand'] = target\n",
    "    except Exception as e:\n",
    "        logger.error(f\"in reduceDimensionality(): {e}\")\n",
    "        \n",
    "# Now Time to call the all function and save it \n",
    "def preprocessFeatures():\n",
    "    global df\n",
    "    try:\n",
    "        add_temporal_features()\n",
    "        add_lag_features()\n",
    "        add_window_features()\n",
    "        add_exp_window_features()\n",
    "        df.dropna(inplace=True)\n",
    "        if df is None or df.empty:\n",
    "            raise ValueError(\"DataFrame is None or empty after dropping missing values.\")\n",
    "        ### call other steps\n",
    "        select_best_features()\n",
    "        normalizeScaling()\n",
    "        reduceDimensionality()\n",
    "    except Exception as e:\n",
    "        logger.error(f'in preprocessFeatures(): {e}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #Loading data\n",
    "    df = getData(r'../data/2022.csv')\n",
    "    # Get the cleaned data\n",
    "    dataCleaning()\n",
    "    # Get processed data with feature selection\n",
    "    preprocessFeatures()\n",
    "    if df is not None:\n",
    "        # Save the processed data\n",
    "        output_file_path = r\"C:/Users/SRA/Desktop/backup/C/MLgrit/time_series_project/uber-taxi-demand/data/featurePipelineFinalData.parquet\"\n",
    "        df.dropna(axis=0, inplace=True)\n",
    "        df.to_parquet(output_file_path, index=False)\n",
    "        print(f\"data has been saved successfully!\")\n",
    "    else:\n",
    "        print(\"No valid processed data to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning_Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow.sklearn\n",
    "import mlflow\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "import logging\n",
    "\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%d-%b(%m)-%Y %I:%M:%S',\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#Loading data\n",
    "def getData(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = pd.read_parquet(path)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"in getData(): {e}\")\n",
    "\n",
    "def splitting() -> tuple:\n",
    "    global df\n",
    "    try:\n",
    "        X = df.drop(columns=[\"taxi_demand\",])\n",
    "        y = df.taxi_demand\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    except Exception as e:\n",
    "        logger.error(f'in splitting(): {e}')\n",
    "\n",
    "def myModelxgb(X_train, X_test, y_train, y_test) -> xgb.XGBRegressor:\n",
    "    try:\n",
    "        mlflow.set_experiment(\"TimeSeries\")\n",
    "        with mlflow.start_run():\n",
    "            x_model = xgb.XGBRegressor()\n",
    "            param_dist = {\n",
    "                'max_depth': randint(1, 16),\n",
    "                'n_estimators': randint(100, 600),\n",
    "                'min_child_weight': randint(1, 16),\n",
    "                'gamma': [0, 0.1, 0.2],\n",
    "                'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "                'nthread': randint(1, 16),\n",
    "            }\n",
    "            # run a randomized search\n",
    "            n_iter_search = 20\n",
    "            random_search = RandomizedSearchCV(x_model, param_distributions=param_dist,\n",
    "                                               n_iter=n_iter_search, random_state=42)\n",
    "            # fit the model\n",
    "            random_search.fit(X_train, y_train)\n",
    "            # Predict on the test set using the best estimator from the grid search\n",
    "            y_pred = random_search.best_estimator_.predict(X_test)\n",
    "            \n",
    "            # Log parameters \n",
    "            mlflow.log_params(random_search.best_params_)\n",
    "            # Calculate and log the evaluation metric (e.g., RMSE)\n",
    "            rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "            mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "            #Log Matrics\n",
    "            mlflow.log_metrics({\n",
    "                \"RMSE\": rmse,\n",
    "                \"MAE\": mae,\n",
    "                \"MAPE0\": mape,\n",
    "                \"R2_SCORE\": r2\n",
    "            })\n",
    "\n",
    "            # Saving the best model obtained after hyperparameter tuning\n",
    "            mlflow.sklearn.log_model(random_search.best_estimator_, 'XGBoost_best_model')\n",
    "\n",
    "            return random_search.best_estimator_\n",
    "    except Exception as e:\n",
    "        logger.error(f\"in myModelxgb(): {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = getData(r'../data/featurePipelineFinalData.parquet')\n",
    "    X_train, X_test, y_train, y_test = splitting()\n",
    "    best_model = myModelxgb(X_train, X_test, y_train, y_test)\n",
    "    # we have to use 'best_model' for further predictions or inference"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
