{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature_Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from feature_engine.datetime import DatetimeFeatures\n",
    "from feature_engine.selection import SmartCorrelatedSelection, RecursiveFeatureElimination\n",
    "from feature_engine.timeseries.forecasting import LagFeatures, WindowFeatures, ExpandingWindowFeatures\n",
    "\n",
    "### Logging Configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt= '%d-%b(%m)-%Y %I:%M:%S',\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#Lodding Data\n",
    "def getData(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"in getData(): {e}\")\n",
    "\n",
    "def dataCleaning() -> None:\n",
    "    global df\n",
    "    try:\n",
    "        df['timestamp'] = pd.to_datetime(df.tpep_pickup_datetime)\n",
    "        df.drop(columns=['tpep_pickup_datetime'], inplace= True)\n",
    "        df.drop_duplicates(subset= ['timestamp'], inplace=True)\n",
    "        df = df[~(df.timestamp > pd.Timestamp('2022-12-31 00:00:00'))]\n",
    "    except Exception as e:\n",
    "        logger.error(f'in dataCleaning(): {e}')\n",
    "        \n",
    "# featureEngineering start here\n",
    "def add_temporal_features() -> None:\n",
    "    global df\n",
    "    try:\n",
    "        features_to_extract = [\n",
    "            \"month\", \"quarter\", \"semester\", \"year\", \"week\", \"day_of_week\", \"day_of_month\",\n",
    "            \"day_of_year\", \"weekend\", \"month_start\", \"month_end\", \"quarter_start\",\n",
    "            \"quarter_end\", \"year_start\", \"year_end\", \"leap_year\", \"days_in_month\", \"hour\", \"minute\", \"second\"\n",
    "        ]\n",
    "        temporal = DatetimeFeatures(features_to_extract=features_to_extract).fit_transform(df[['timestamp']])\n",
    "        for col in temporal.columns:\n",
    "            df.loc[:, col] = temporal[col].values\n",
    "    except Exception as e:\n",
    "        logger.error(f'in add_temporal_features(): {e}')\n",
    "\n",
    "def add_lag_features() -> None:\n",
    "    global df\n",
    "    try:\n",
    "        lagfeatures = LagFeatures(variables=None, periods=[1, 2, 4, 8, 16, 24], freq=None, sort_index=True,\n",
    "                                missing_values='raise', drop_original=False)\n",
    "        lagfeatures.fit(df[['timestamp', 'passenger_demand', 'taxi_demand']])\n",
    "        features = lagfeatures.transform(df[['timestamp', 'passenger_demand', 'taxi_demand']])\n",
    "        for col in list(features.columns)[3:]:\n",
    "            df[col] = features[col].values\n",
    "    except Exception as e:\n",
    "        logger.error(f'in The add_lag_features(): {e}')\n",
    "        \n",
    "def add_window_features() -> None:\n",
    "    global df\n",
    "    try:\n",
    "        window = WindowFeatures(\n",
    "            variables=None, window=7, min_periods=1,\n",
    "            functions=['mean', 'std', 'median'], periods=1, freq=None, sort_index=True,\n",
    "            missing_values='raise', drop_original=False\n",
    "        )\n",
    "        window.fit(df[['timestamp', 'passenger_demand', 'taxi_demand']])\n",
    "        features = window.fit_transform(df[['timestamp', 'passenger_demand', 'taxi_demand']])\n",
    "        for col in list(features.columns)[3:]:\n",
    "            df[col] = features[col].values\n",
    "    except Exception as e:\n",
    "        logger.error(f'in add_window_features(): {e}')\n",
    "        \n",
    "def add_exp_window_features() -> None:\n",
    "    global df\n",
    "    try:\n",
    "        expwindow = ExpandingWindowFeatures(\n",
    "            variables=None, min_periods=None, functions='std',\n",
    "            periods=1, freq=None, sort_index=True,\n",
    "            missing_values='raise', drop_original=False\n",
    "        )\n",
    "        expwindow.fit(df[['timestamp', 'passenger_demand', 'taxi_demand']])\n",
    "        features = expwindow.fit_transform(df[['timestamp', 'passenger_demand', 'taxi_demand']])\n",
    "        for col in list(features.columns)[3:]:\n",
    "            df[col] = features[col].values\n",
    "    except Exception as e:\n",
    "        logger.error(f'in add_exp_window_features(): {e}')\n",
    "        \n",
    "# Feture Selection Start here\n",
    "def select_best_features():\n",
    "    global df\n",
    "    try:\n",
    "        X = df.drop(columns=['timestamp','passenger_demand', 'taxi_demand'])\n",
    "        y = df['taxi_demand']\n",
    "        scs = SmartCorrelatedSelection(\n",
    "            variables=None, method='pearson', threshold=0.5,\n",
    "            missing_values='ignore', selection_method='variance',\n",
    "            confirm_variables=False\n",
    "        )\n",
    "        scs_columns = set(scs.fit_transform(X).columns)\n",
    "        rfe = RecursiveFeatureElimination(\n",
    "            DecisionTreeRegressor(max_depth=3), scoring='r2', cv=3, threshold=0.01,\n",
    "            variables=None, confirm_variables=False\n",
    "        )\n",
    "        rfe_columns = rfe.fit_transform(X, y)\n",
    "        scs_columns.update(rfe_columns)\n",
    "        df = df[list(scs_columns)]\n",
    "        df['taxi_demand'] = y\n",
    "    except Exception as e:\n",
    "        logger.error(f'in select_best_features(): {e}')\n",
    "\n",
    "# Data Scaling here\n",
    "def normalizeScaling() -> None:\n",
    "    global df\n",
    "    try:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df.drop(columns=['taxi_demand',]))\n",
    "        df.loc[:, df.columns[:-1]] = scaler.transform(df.drop(columns=['taxi_demand',])) \n",
    "    except Exception as e:\n",
    "        logger.error(f\"in normalizeScaling(): {e}\")\n",
    "        \n",
    "# DimensonalRedaction start from here\n",
    "def reduceDimensionality() -> None:\n",
    "    global df\n",
    "    try:\n",
    "        features = df.drop(columns=['taxi_demand'])\n",
    "        target = df['taxi_demand']\n",
    "        pca = PCA(n_components=19)\n",
    "        features_reduced = pca.fit_transform(features)\n",
    "        df = pd.DataFrame(features_reduced, columns=[f'PC{i}' for i in range(1, 20)])\n",
    "        df['taxi_demand'] = target\n",
    "    except Exception as e:\n",
    "        logger.error(f\"in reduceDimensionality(): {e}\")\n",
    "        \n",
    "# Now Time to call the all function and save it \n",
    "def preprocessFeatures():\n",
    "    global df\n",
    "    try:\n",
    "        add_temporal_features()\n",
    "        add_lag_features()\n",
    "        add_window_features()\n",
    "        add_exp_window_features()\n",
    "        df.dropna(axis=0,inplace=True)\n",
    "        if df is None or df.empty:\n",
    "            raise ValueError(\"DataFrame is None or empty after dropping missing values.\")\n",
    "        ### call other steps\n",
    "        select_best_features()\n",
    "        normalizeScaling()\n",
    "        reduceDimensionality()\n",
    "    except Exception as e:\n",
    "        logger.error(f'in preprocessFeatures(): {e}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #Loading data\n",
    "    df = getData(r'../data/2022.csv')\n",
    "    # Get the cleaned data\n",
    "    dataCleaning()\n",
    "    # Get processed data with feature selection\n",
    "    preprocessFeatures()\n",
    "    if df is not None:\n",
    "        # Save the processed data\n",
    "        output_file_path = r\"C:/Users/SRA/Desktop/backup/C/MLgrit/time_series_project/uber-taxi-demand/data/featurePipelineFinalData.parquet\"\n",
    "        df.dropna(axis=0, inplace=True)\n",
    "        df.to_parquet(output_file_path, index=False)\n",
    "        print(f\"data has been saved successfully!\")\n",
    "    else:\n",
    "        print(\"No valid processed data to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning_Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mlflow.sklearn\n",
    "import mlflow\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "import logging\n",
    "\n",
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%d-%b(%m)-%Y %I:%M:%S',\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "#Loading data\n",
    "def getData(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = pd.read_parquet(path)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"in getData(): {e}\")\n",
    "\n",
    "def splitting() -> tuple:\n",
    "    global df\n",
    "    try:\n",
    "        X = df.drop(columns=[\"taxi_demand\",])\n",
    "        y = df.taxi_demand\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    except Exception as e:\n",
    "        logger.error(f'in splitting(): {e}')\n",
    "\n",
    "def myModelxgb(X_train, X_test, y_train, y_test) -> xgb.XGBRegressor:\n",
    "    try:\n",
    "        mlflow.set_experiment(\"TimeSeries\")\n",
    "        with mlflow.start_run():\n",
    "            x_model = xgb.XGBRegressor()\n",
    "            param_dist = {\n",
    "                'max_depth': randint(1, 16),\n",
    "                'n_estimators': randint(100, 600),\n",
    "                'min_child_weight': randint(1, 16),\n",
    "                'gamma': [0, 0.1, 0.2],\n",
    "                'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "                'nthread': randint(1, 16),\n",
    "            }\n",
    "            # run a randomized search\n",
    "            n_iter_search = 20\n",
    "            random_search = RandomizedSearchCV(x_model, param_distributions=param_dist,\n",
    "                                               n_iter=n_iter_search, random_state=42)\n",
    "            # fit the model\n",
    "            random_search.fit(X_train, y_train)\n",
    "            # Predict on the test set using the best estimator from the grid search\n",
    "            y_pred = random_search.best_estimator_.predict(X_test)\n",
    "            \n",
    "            # Log parameters \n",
    "            mlflow.log_params(random_search.best_params_)\n",
    "            # Calculate and log the evaluation metric (e.g., RMSE)\n",
    "            rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "            mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "            #Log Matrics\n",
    "            mlflow.log_metrics({\n",
    "                \"RMSE\": rmse,\n",
    "                \"MAE\": mae,\n",
    "                \"MAPE0\": mape,\n",
    "                \"R2_SCORE\": r2\n",
    "            })\n",
    "\n",
    "            # Saving the best model obtained after hyperparameter tuning\n",
    "            mlflow.sklearn.log_model(random_search.best_estimator_, 'XGBoost_best_model')\n",
    "\n",
    "            return random_search.best_estimator_\n",
    "    except Exception as e:\n",
    "        logger.error(f\"in myModelxgb(): {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = getData(r'../data/featurePipelineFinalData.parquet')\n",
    "    X_train, X_test, y_train, y_test = splitting()\n",
    "    best_model = myModelxgb(X_train, X_test, y_train, y_test)\n",
    "    # we have to use 'best_model' for further predictions or inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "      <th>PC9</th>\n",
       "      <th>PC10</th>\n",
       "      <th>PC11</th>\n",
       "      <th>PC12</th>\n",
       "      <th>PC13</th>\n",
       "      <th>PC14</th>\n",
       "      <th>PC15</th>\n",
       "      <th>PC16</th>\n",
       "      <th>PC17</th>\n",
       "      <th>PC18</th>\n",
       "      <th>PC19</th>\n",
       "      <th>taxi_demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.718273</td>\n",
       "      <td>14.930842</td>\n",
       "      <td>25.355853</td>\n",
       "      <td>8.588989</td>\n",
       "      <td>6.786783</td>\n",
       "      <td>-10.252492</td>\n",
       "      <td>-6.724879</td>\n",
       "      <td>-3.978018</td>\n",
       "      <td>-4.377113</td>\n",
       "      <td>0.737261</td>\n",
       "      <td>-15.359071</td>\n",
       "      <td>-0.060347</td>\n",
       "      <td>3.371094e-18</td>\n",
       "      <td>-1.369342e-18</td>\n",
       "      <td>-1.577304e-18</td>\n",
       "      <td>4.766313e-18</td>\n",
       "      <td>2.482778e-18</td>\n",
       "      <td>2.035194e-18</td>\n",
       "      <td>2.022135e-19</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.807196</td>\n",
       "      <td>-0.702940</td>\n",
       "      <td>-2.078457</td>\n",
       "      <td>-0.502498</td>\n",
       "      <td>-0.952596</td>\n",
       "      <td>-1.075451</td>\n",
       "      <td>-0.169224</td>\n",
       "      <td>0.961579</td>\n",
       "      <td>-0.357883</td>\n",
       "      <td>-1.981192</td>\n",
       "      <td>-0.292281</td>\n",
       "      <td>2.143428</td>\n",
       "      <td>-3.267452e-16</td>\n",
       "      <td>1.856613e-16</td>\n",
       "      <td>2.516799e-16</td>\n",
       "      <td>-6.478800e-16</td>\n",
       "      <td>-1.491158e-16</td>\n",
       "      <td>-3.659323e-16</td>\n",
       "      <td>-1.940145e-16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.390297</td>\n",
       "      <td>4.294840</td>\n",
       "      <td>4.633114</td>\n",
       "      <td>-0.524657</td>\n",
       "      <td>-2.017603</td>\n",
       "      <td>4.936140</td>\n",
       "      <td>5.132366</td>\n",
       "      <td>-6.536679</td>\n",
       "      <td>-7.906023</td>\n",
       "      <td>-2.232191</td>\n",
       "      <td>8.965390</td>\n",
       "      <td>0.441244</td>\n",
       "      <td>3.222898e-17</td>\n",
       "      <td>-1.141686e-16</td>\n",
       "      <td>4.548973e-16</td>\n",
       "      <td>-6.967821e-16</td>\n",
       "      <td>2.830278e-16</td>\n",
       "      <td>-4.624935e-16</td>\n",
       "      <td>-1.881244e-16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.319921</td>\n",
       "      <td>1.029894</td>\n",
       "      <td>-0.167156</td>\n",
       "      <td>-0.922772</td>\n",
       "      <td>-1.066377</td>\n",
       "      <td>1.126567</td>\n",
       "      <td>0.672173</td>\n",
       "      <td>4.652307</td>\n",
       "      <td>5.595849</td>\n",
       "      <td>1.065017</td>\n",
       "      <td>-0.067521</td>\n",
       "      <td>1.039021</td>\n",
       "      <td>-5.239083e-16</td>\n",
       "      <td>4.357634e-16</td>\n",
       "      <td>-3.399934e-16</td>\n",
       "      <td>9.088027e-17</td>\n",
       "      <td>-4.793232e-16</td>\n",
       "      <td>2.390657e-17</td>\n",
       "      <td>-1.896852e-16</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18.320177</td>\n",
       "      <td>9.328241</td>\n",
       "      <td>14.014048</td>\n",
       "      <td>3.330964</td>\n",
       "      <td>1.054496</td>\n",
       "      <td>-1.995706</td>\n",
       "      <td>0.691989</td>\n",
       "      <td>-1.674017</td>\n",
       "      <td>-2.040926</td>\n",
       "      <td>-1.952490</td>\n",
       "      <td>22.957588</td>\n",
       "      <td>-0.450683</td>\n",
       "      <td>-1.105785e-17</td>\n",
       "      <td>5.503029e-17</td>\n",
       "      <td>-2.298146e-16</td>\n",
       "      <td>3.555405e-16</td>\n",
       "      <td>-1.377897e-16</td>\n",
       "      <td>2.342995e-16</td>\n",
       "      <td>9.436554e-17</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         PC1        PC2        PC3       PC4       PC5        PC6       PC7  \\\n",
       "0  23.718273  14.930842  25.355853  8.588989  6.786783 -10.252492 -6.724879   \n",
       "1   3.807196  -0.702940  -2.078457 -0.502498 -0.952596  -1.075451 -0.169224   \n",
       "2  12.390297   4.294840   4.633114 -0.524657 -2.017603   4.936140  5.132366   \n",
       "3   7.319921   1.029894  -0.167156 -0.922772 -1.066377   1.126567  0.672173   \n",
       "4  18.320177   9.328241  14.014048  3.330964  1.054496  -1.995706  0.691989   \n",
       "\n",
       "        PC8       PC9      PC10       PC11      PC12          PC13  \\\n",
       "0 -3.978018 -4.377113  0.737261 -15.359071 -0.060347  3.371094e-18   \n",
       "1  0.961579 -0.357883 -1.981192  -0.292281  2.143428 -3.267452e-16   \n",
       "2 -6.536679 -7.906023 -2.232191   8.965390  0.441244  3.222898e-17   \n",
       "3  4.652307  5.595849  1.065017  -0.067521  1.039021 -5.239083e-16   \n",
       "4 -1.674017 -2.040926 -1.952490  22.957588 -0.450683 -1.105785e-17   \n",
       "\n",
       "           PC14          PC15          PC16          PC17          PC18  \\\n",
       "0 -1.369342e-18 -1.577304e-18  4.766313e-18  2.482778e-18  2.035194e-18   \n",
       "1  1.856613e-16  2.516799e-16 -6.478800e-16 -1.491158e-16 -3.659323e-16   \n",
       "2 -1.141686e-16  4.548973e-16 -6.967821e-16  2.830278e-16 -4.624935e-16   \n",
       "3  4.357634e-16 -3.399934e-16  9.088027e-17 -4.793232e-16  2.390657e-17   \n",
       "4  5.503029e-17 -2.298146e-16  3.555405e-16 -1.377897e-16  2.342995e-16   \n",
       "\n",
       "           PC19  taxi_demand  \n",
       "0  2.022135e-19          NaN  \n",
       "1 -1.940145e-16          NaN  \n",
       "2 -1.881244e-16          NaN  \n",
       "3 -1.896852e-16          NaN  \n",
       "4  9.436554e-17          NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(r'../data/feature-2023.parquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeseriesENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
